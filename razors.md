# Data Engineering Razors

ğŸª’ Define your workflow in a DAG.

ğŸª’ Minimize the responsibility of each DAG task.

ğŸª’ Window functions will save you.

ğŸª’ Use scheduled run timestamp.

ğŸª’ Creating columns to understand transformations is like print debugging... remove them in prod.

ğŸª’ Operate on ETL windows defined by the scheduled run timestamp.

ğŸª’ Delete before you write.

ğŸª’ Don't shuffle data unless it's required.

ğŸª’ Build layers of aggregation: telemetry -> session -> ETL aggregate -> ...

ğŸª’ Modularize your code into reusable components (UDFs are your friend here).

ğŸª’ Unit test these components.

ğŸª’ Test the pipeline end-to-end with data that mimics your source data.

ğŸª’ Make sure there is logging for each DAG task and run/scheduled timestamps.

ğŸª’ Keep your documentation close to your code.

ğŸª’ Create alerts on failure.

ğŸª’ Operate on a unit of time that is acceptable given the stakeholder's request _and_ the scale of the data.

ğŸª’ Choose a smaller unit of time rather than larger (e.g. hourly over daily).

ğŸª’ Clean up after yourself (e.g. with try/except clauses in Python).

ğŸª’ Keep writes idempotent.

ğŸª’ Let your pipeline do some (but not necessarily all) backfill.

ğŸª’ Utilize partitioning built into your framework.

ğŸª’ Don't wrap SQL transformations in strings.

ğŸª’ Your prod role should be different than the role you develop with.

ğŸª’ Discuss the schema with stakeholders before you start.

ğŸª’ Use tools/frameworks that abstract your architecture rather than having one tool that does everything (where possible).

ğŸª’ If storage costs < compute costs then it makes sense to pre-compute and store bigger tables, rather than doing this on the fly.